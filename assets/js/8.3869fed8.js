(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{427:function(e,t,a){"use strict";a.r(t);var r=a(53),n=Object(r.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"backend"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#backend"}},[e._v("#")]),e._v(" Backend")]),e._v(" "),a("p",[e._v("The following documentation is structured after the CRISP-DM process.")]),e._v(" "),a("p",[a("img",{attrs:{src:"/CRISP_DM.png?raw=true",alt:"CRISP"}}),e._v(" "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining#/media/File:CRISP-DM_Process_Diagram.png",target:"_blank",rel:"noopener noreferrer"}},[e._v("Source of the original image"),a("OutboundLink")],1)]),e._v(" "),a("blockquote",[a("p",[a("strong",[e._v("NOTE")]),e._v(": For the first step in the CRISP-DM, visit the "),a("a",{attrs:{href:"/backend"}},[a("em",[e._v("Design Thinking and Process")])]),e._v(" page\nand for the second step the "),a("a",{attrs:{href:"/datasource"}},[a("em",[e._v("Data Source")])]),e._v(" section.")])]),e._v(" "),a("h2",{attrs:{id:"_3-data-preparation-speed-layer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-data-preparation-speed-layer"}},[e._v("#")]),e._v(" 3. Data Preparation - Speed Layer")]),e._v(" "),a("p",[e._v("The following paragraph will focus on the data preparation and aggreagtion as it is performed in the speed layer.\nThis closly represents the Data Preparation phase of the CRISP-DM approach. However, it does not cover the process for selecting relevant variables during development of the solution. A few remarks on this step will be given after the documentation of the speed layer.")]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"input"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#input"}},[e._v("#")]),e._v(" Input")]),e._v(" "),a("p",[e._v("The Speed Layer processes the simulation-data which is consumed from a Kafka-Stream.\nAfter parsing the JSON-data, it gets converted to a pyspark-DataFrame (pyspark.sql.DataFrame).")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('df = spark.readStream.format("kafka") \\\n    .option("kafka.bootstrap.servers", "kafka:9092") \\\n    .option("subscribe", "simulation") \\\n    .load()\n\ndf = df.selectExpr("CAST(value AS STRING)")\n\ndf = df.withColumn("value", from_json("value", schema)) \\\n    .select(col(\'value.*\')) \\\n')])])]),a("h3",{attrs:{id:"transformation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transformation"}},[e._v("#")]),e._v(" Transformation")]),e._v(" "),a("p",[e._v("In the transformation phase, the data goes through several steps: Filtering, Harmonization, Aggregation, Enrichment. The Filtering- and Harmonization-Phase are less needed due to the fact that most of the data comes from the same simulated data source and is pretty much homogeneous. For real world data, this processing step would have to be extended.")]),e._v(" "),a("h4",{attrs:{id:"aggregation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#aggregation"}},[e._v("#")]),e._v(" Aggregation")]),e._v(" "),a("p",[e._v("The incoming data-stream gets aggregated by each trip.")]),e._v(" "),a("ul",[a("li",[e._v("Master-data (such as the corresponding route, truck or trip) is saved once.")]),e._v(" "),a("li",[e._v("Moving-data is converted to time series. This enables the resource efficient historization of the data for later use in the Batch Layer. The time series are of use for computing metrics like averages, moving averages (not implemented in the latest version), standard-deviations (not implemented), and for deriving most recent values, which are used in the frontend.")]),e._v(" "),a("li",[e._v("As mentioned above, moving data for which only the last value is of relevance to the frontend is saved in a separate field to be forwarded to the frontend api. Some constantly changing values which are not of importance for machine learning (such as the weather, which currently has no influence on the simulation) are treated the same way to save on resources. In a production environment, it would be a good idea to save everything to the batch database in case this data is needed in the future for different use cases.")])]),e._v(" "),a("p",[e._v("An example of all three aggregation types:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('df_serve = df_serve \\\n    .groupBy(\'tripId\') \\\n    .agg(F.first("truckId").alias("sd_truckId"), \\\n    F.collect_list(\'speed\').alias("ts_speed"), \\\n    F.avg(\'consumption\').alias("agg_last_consumption"))\n')])])]),a("h4",{attrs:{id:"enrichment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#enrichment"}},[e._v("#")]),e._v(" Enrichment")]),e._v(" "),a("p",[e._v("Additional master-data is joined, such as the length of each route, to compute metrics like the current progress per trip. The length of each route is a static dataframe that does not require recomputation in the batch-layer. Rather, this dataframe would have to be updated by a route planning system as soon as a new route is entered into the system.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('AS.download_file(save_path, cloud_file_name, container_name)\nroute_dist_pd = pd.read_csv("./route_dist.csv")\nroute_dist = sqlCtx.createDataFrame(route_dist_pd)\ndf_serve = df_serve.join(route_dist, \'sd_routeId\', how=\'left_outer\')\n\n@udf("array<integer>")  \ndef PRODUCE_ts_agg_acc_meters(xs):  \n    if xs:  \n        temp = [int(round(sum(xs[0:i]), 0)) \\\n        for i in (range(1, len(xs) + 1))]  \n        return temp\n    \ndf_serve = df_serve \\\n    .withColumn("ts_agg_acc_meters", \\\n    PRODUCE_ts_agg_acc_meters("ts_meters_intervall"))\n')])])]),a("p",[e._v("Historical aggregates, stemming from the Batch Layer, are used to compute Delta-KPIs. Those fulfill the purpose to better assess current data, when seeing it in relation to values which can be considered normal for each route. For this, similar to joining master data above, aggregation data is joined and then used set current values into comparison. The aggregations are periodically recomputed by the batch layer to ensure that averages are up to date.")]),e._v(" "),a("p",[e._v("External datasources, such as the current weather for each truck, get joined as additional data for the frontend. A custom weather api caller was defined in order to obtain weather codes in concordance with the frontend specification. Additional information, such as a description of the weather, the temperature or humidity would also be available.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('@udf("integer")\ndef get_weather_code_for_location(lat, lon, unix_time):\n    result = Weather.get_weather_for_location(lat, lon, unix_time)\n    return result[0]\n\nif (enrich_weather_data):\n    try:\n        df_serve = df_serve \\\n            .withColumn("weatherCode", \\\n            get_weather_code_for_location("lat","lon","timeStamp"))\n')])])]),a("p",[e._v("Machine Learning Models are used as inference-engines, to enrich the streaming data with predictions, such as the delay per trip. The models are fetched from Azure Blob Storage. A simplified code fragment showing the general process:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('newest_filename, s = AS.get_newest_file(container_name, arrival_file_substring)\nfilepath, s = AS.download_file(save_path, newest_filename, container_name)\narrival_model_path = AS.unzip(".", filepath)\nmodel_arrival = PipelineModel.read().load(arrival_model_path)\ndf_serve = model_arrival.transform(df_serve)\n')])])]),a("h3",{attrs:{id:"output"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#output"}},[e._v("#")]),e._v(" Output")]),e._v(" "),a("p",[e._v("There are two outputs of the Speed Layer.\nA subset of all features (mostly aggregates such as averages or last values and identifying labels) get forwarded to the frontend via an Kafka-Stream.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('df_to_frontend_stream = df_to_frontend \\\n    .writeStream \\\n    .format("kafka") \\\n    .option("kafka.bootstrap.servers", "kafka:9092") \\\n    .option("topic", "frontend") \\\n    .outputMode("Complete") \\\n    .option("checkpointLocation", False) \\\n    .start()\n')])])]),a("p",[e._v("A sample of one message to the frontend:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('{"trip_id":"VA1Z9OT98P","truck_id":"5f070922c92647260c2b2886","number_plate":"S HZ 8245","route_id":"5f070922c92647260c2b287e","truck_mass":14070,"departure":"Muenchen","arrival":"Stuttgart","departure_time":1595456465,"telemetry_timestamp":1595456465,"arrival_time":1595466901,"telemetry_lat":48.16400894659716,"telemetry_lon":11.483320377669836,"truck_speed":13.08,"truck_consumption":6.66,"avg_truck_speed":13.08,"truck_acceleration":-6.07,"avg_truck_acceleration":-6.07,"normal_consumption":6.66,"route_progress":0,"driver_class":1,"delay":-1773,"driver_acceleration":2,"driver_speed":2,"driver_brake":0,"incident":false,"truck_condition":1,"tires_efficiency":0,"engine_efficiency":0,"weather":0,"year":2008,"arrived":0,"truck_type":"LOCAL","driver_duration":0,"road_type":"INTERURBAN","next_service":212,"service_interval":9000}\n')])])]),a("p",[e._v("Secondly, all master data along with all time series is saved to the batch layer, by inserting it into the persistence-layer (Cosmos-DB) in a historized fashion. We decided to insert the already preprocessed and enriched data into the batch-db (and not directly from the simulation) to ensure consistency in terms of enrichment steps. Therefore, the foreachBatch()-Method was used, in order to insert data via the pymongo library.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('def write_to_mongo(df_to_batch, epoch_id):\n    tempdataframe = df_to_batch.toPandas()\n        with pymongo.MongoClient(uri) as client:\n            mydb = client[mongodb_name]\n            sparkcollection = mydb[mongodb_collection]\n            df_json = json.loads(tempdataframe.T.to_json()).values()\n            try:\n                sparkcollection.insert(df_json)\n\ndf_to_batch_stream = df_to_batch.writeStream \\\n    .outputMode("Update") \\\n    .foreachBatch(write_to_mongo) \\\n    .start()\n')])])]),a("p",[e._v("A major hurdle was inserting the label for time series into the database. The dependent variable for the estimated delay prediction is calculated using the accumulated amount of seconds per trip. This value is only available with the last signal for each trip, as soon as a truck has arrived at its destination, but has to be inserted for all previous tuples of the corresponding trip, which have already been saved to the database at that point. Updating the documents in the batch-db has to be done to allow the training of models, eligible of predicting the label for each step of the time series.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('sparkcollection.update_many( \\\n\t        {"tripId": temp_tripId}, \\\n\t\t    {"$set": {"LABEL_final_agg_acc_sec": temp_LABEL}})\n')])])]),a("hr"),e._v(" "),a("p",[e._v("As the data stems from a simulation, data cleaning did not have to be performed.\nHowever, a large problem that was faced during development was the constantly changing nature of the data,\nas the development of the simulation was simultaneous to the development of the backend.\nIt would have been better to have a fixed data source from the start. This would have enabled structured work, more consistent with CRISP-DM.")]),e._v(" "),a("h2",{attrs:{id:"_4-modeling-batch-layer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-modeling-batch-layer"}},[e._v("#")]),e._v(" 4. Modeling - Batch Layer")]),e._v(" "),a("p",[e._v("In the modeling phase, we ended up with two models. One supervised, and one unsupervised approach.")]),e._v(" "),a("p",[e._v("We clustered the driver-habits using the spark-proprietary K-Means-Clustering-Algorithm and evaluated the models with Silhouettes. Incremental-Model-Training / Online-Training was considered for the clustering, but could not yet be implemented.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('CL_kmeans = KMeans() \\\n\t    .setK(3) \\\n\t    .setSeed(1) \\\n\t    .setFeaturesCol("scaledFeatures")\n')])])]),a("p",[e._v("Gradient Boosted Trees (GMT) were utilized to regress on the delay per trip and evaluated using the Mean Absolute Error.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('EA_gbt = GBTRegressor(labelCol="label", featuresCol="features")\n')])])]),a("p",[e._v("The GBT-Algorithm was challenged against LSTM-Networks, which held the potential to make better use of Time Series Data. However, while the data is available in time series format, the prediction is a regression task and heavily profits from the ability of GBT to learn different patterns in data. Unfortunately the LSTM, implemented with Keras and Tensoflow as Backend, was not able to achieve better results than the sequentially growing decision trees.\nIn the next iteration of the data science process, it could be tried to predict the delay using a multi model or multi input model approach with LSTM's and dense networks, concatenating several input streams with different dimensionality. This would not only process time-series, but also one-dimensional master data, which turned out to be a good predictor with the GBT. Anyway, the LSTM code can be found in /Backend/batchlayer/NN_Test.py.")]),e._v(" "),a("p",[e._v("For both models, hyperparameters were optimized with the help of a Grid Search. The grid could be extended by further variables if better results are required, it is however more likely that training data amounts and data preparation play a bigger role in the quality of the models.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("EA_paramGrid = ParamGridBuilder() \\\n        .addGrid(EA_gbt.maxDepth, [3, 5, 7]) \\\n        .addGrid(EA_gbt.maxIter, [10, 20, 50, 100]) \\\n        .addGrid(EA_gbt.stepSize, [0.001, 0.01, 0.1, 1]) \\\n        .build()\n")])])]),a("h2",{attrs:{id:"_5-evaluation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-evaluation"}},[e._v("#")]),e._v(" 5. Evaluation")]),e._v(" "),a("h3",{attrs:{id:"evaluation-of-machine-learning-models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#evaluation-of-machine-learning-models"}},[e._v("#")]),e._v(" Evaluation of Machine Learning Models")]),e._v(" "),a("p",[e._v("Each parameter combination in our grid is evaluated using  10-Fold-Cross-Validation, due to the fact that is an sufficiently big and decorrelated estimate of our error-metric.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v("EA_pipeline=Pipeline().setStages([EA_indexer1,\\\n\t\t\t    EA_indexer2, \\\n\t\t\t    EA_vectorAssembler, \\\n\t\t\t    EA_gbt])\n\nEA_crossval = CrossValidator(estimator=EA_pipeline, \\\n                          estimatorParamMaps=EA_paramGrid, \\\n                          evaluator=RegressionEvaluator(), \\\n                          numFolds=10)\n\nEA_crossval = EA_crossval.fit(sql_df_trips) \nEA_best_model = EA_crossval.bestModel\n")])])]),a("p",[e._v("Lastly, we found heteroscedasticity in our prediction-error, meaning that the variance of our error-estimate was not constant. We were not able to eliminate this issue yet.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",[a("code",[e._v('jointplot= sns.jointplot(x="label", \\ \n\t\t    y="pred", \\\n\t\t    data=sql_df_trips)\n')])])]),a("h2",{attrs:{id:"_6-deployment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-deployment"}},[e._v("#")]),e._v(" 6. Deployment")]),e._v(" "),a("blockquote",[a("p",[a("strong",[e._v("NOTE")]),e._v(": For a detailed description of how to deploy Horizon yourself, please see the tab "),a("a",{attrs:{href:"/instructions"}},[e._v("How to deploy")]),e._v(".")])]),e._v(" "),a("p",[e._v("Before beginning work on Horizon, Microsoft Azure was chosen as the platform on which it would later be deployed.\nThis is for two reasons:\nFirstly, Azure is currently shaping to be the main competitor to the industry leading Amazon Web Services and offers a free student credit program, making it well suited for a study project.\nSecondly, while some team-members had previous experience with Googles Cloud Platform, another option with a student credit offering, we wanted to gain experience with another solution.")]),e._v(" "),a("p",[e._v("We did not expect that hosting our solution would require as many resources, but in the end the student credit of multiple accounts was fully used up.")]),e._v(" "),a("p",[e._v("Azure Kubernetes Service and Azure Cosmos DB were chosen early in the process as the two main components on which Horizon runs. Subsequent architecture and tool decisions were made accordingly.")]),e._v(" "),a("p",[e._v("After testing parts of the solution in clusters or containers on local machines, the live deployment only required small changes to the general workflow, the largest being heavy wait times while waiting for container images to upload for live testing.")]),e._v(" "),a("p",[e._v("As Horizon was developed as a prototype and underwent constant changes, in addition to the high cost of hosting the service, Horizon did not run online for extendend periods of time. This means that no large database of data for training could be collected.\nMonitoring was therefore not required (and would have cost additional credit). The Batch Database did not reach full capacity and remained below the free tier of 400 RU/s for the duration of the project.")]),e._v(" "),a("h2",{attrs:{id:"contributors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#contributors"}},[e._v("#")]),e._v(" Contributors")]),e._v(" "),a("p",[a("strong",[e._v("David Rundel")]),e._v(" and "),a("strong",[e._v("Felix Bieswanger")]),e._v(" were mainly involved in the development of the backend and streaming. "),a("strong",[e._v("Jan Anders")]),e._v(" helped with deployment.")])])}),[],!1,null,null,null);t.default=n.exports}}]);